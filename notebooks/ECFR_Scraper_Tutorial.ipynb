{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e623a40",
   "metadata": {},
   "source": [
    "# ECFR Scraper: Getting Started\n",
    "\n",
    "This notebook explains what the ECFR Scraper does and how to use it from the command line and programmatically.\n",
    "\n",
    "- Package: `ecfr_scraper` (run with `python -m ecfr_scraper`)\n",
    "- Console script: `ecfr-scraper` (after `pip install -e .`)\n",
    "- Modules: `ecfr_scraper.scraper`, `ecfr_scraper.metadata`, `ecfr_scraper.utils`, `ecfr_scraper.cli`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5271af",
   "metadata": {},
   "source": [
    "## What it does\n",
    "\n",
    "- Downloads eCFR title XMLs (Titles 1â€“50) from <https://www.govinfo.gov>.\n",
    "- Caches using checksums so unchanged files are skipped.\n",
    "- Optionally parses XML to structured JSON and computes lexical stats.\n",
    "- Extracts per-file metadata (XML, ZIP, TXT; PDF is a placeholder).\n",
    "- Shows progress for multi-title downloads.\n",
    "- Logs to console and `ecfr_scraper.log`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8c913",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Python 3.9 or later\n",
    "- From the repo root, install dependencies:\n",
    "\n",
    "```powershell\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b5319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install requirements into the current kernel\n",
    "%pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7ddc96",
   "metadata": {},
   "source": [
    "## Command-line usage\n",
    "\n",
    "You can run the tool as a module or via the console script (after installing in editable mode).\n",
    "\n",
    "- Show help:\n",
    "\n",
    "```powershell\n",
    "python -m ecfr_scraper -h\n",
    "```\n",
    "\n",
    "- Download and parse a single title:\n",
    "\n",
    "```powershell\n",
    "python -m ecfr_scraper --title 21 --output .\\data --verbose\n",
    "```\n",
    "\n",
    "- Download all titles in parallel (8 workers shown):\n",
    "\n",
    "```powershell\n",
    "python -m ecfr_scraper --all --workers 8 --output .\\data\n",
    "```\n",
    "\n",
    "- Generate only metadata (skip JSON parsing):\n",
    "\n",
    "```powershell\n",
    "python -m ecfr_scraper --title 12 --metadata-only\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CLI help from the notebook (safe, quick)\n",
    "import subprocess\n",
    "subprocess.run([\"python\", \"-m\", \"ecfr_scraper\", \"-h\"], check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c01a8",
   "metadata": {},
   "source": [
    "## Programmatic usage (Python API)\n",
    "\n",
    "Use the `ECFRScraper` class to download titles and parse XML to JSON.\n",
    "This demo keeps it fast: it downloads one title and skips deep parsing by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecfr_scraper.scraper import ECFRScraper\n",
    "from ecfr_scraper.utils import setup_logging\n",
    "import os, json\n",
    "\n",
    "# Configure logging for this session\n",
    "setup_logging(verbose=True)\n",
    "\n",
    "# Choose an output directory (relative to repo root)\n",
    "out_dir = os.path.join(\"..\", \"data\")\n",
    "scraper = ECFRScraper(output_dir=out_dir)\n",
    "\n",
    "# Demo: download a single title (1)\n",
    "xml_path = scraper.download_title_xml(1, out_dir)\n",
    "print(\"XML path:\", xml_path)\n",
    "\n",
    "# Optional: parse XML to JSON (can take some time).\n",
    "# Set this to True to run the parse step.\n",
    "RUN_PARSE = False\n",
    "if RUN_PARSE and xml_path:\n",
    "    data = scraper.parse_xml(xml_path)\n",
    "    if data:\n",
    "        json_path = xml_path.replace(\".xml\", \".json\")\n",
    "        scraper.export_to_json(data, json_path)\n",
    "        print(\"JSON saved:\", json_path)\n",
    "    else:\n",
    "        print(\"Parse failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855e54a7",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "For each downloaded title N:\n",
    "\n",
    "- XML: `./data/titleN.xml`\n",
    "- Parsed JSON: `./data/titleN.json`\n",
    "- File metadata: `./data/titleN.xml.metadata.json`\n",
    "- Global checksums: `./checksums.json`\n",
    "- Logs: `./ecfr_scraper.log`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dccca07",
   "metadata": {},
   "source": [
    "## Advanced usage\n",
    "\n",
    "- Download multiple titles in parallel using the CLI `--all` and `--workers`.\n",
    "- Programmatically, you can call `download_all_titles(max_workers=8)` and then `process_downloaded_files(...)` to parse and export JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201df6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example (commented to avoid long runs):\n",
    "# files = scraper.download_all_titles(out_dir, max_workers=8)\n",
    "# results = scraper.process_downloaded_files(files)\n",
    "# print(results[:2])  # show first few results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608fab31",
   "metadata": {},
   "source": [
    "## Extending\n",
    "\n",
    "- Add more metadata handlers in `MetadataExtractor` (e.g., real PDF parsing with `pypdf` or `pdfminer.six`).\n",
    "- Improve XML parsing (`ECFRScraper.parse_xml`) for namespace-aware parsing or richer structure.\n",
    "- Enhance retries/backoff for HTTP requests."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
